---
title: "Multivaraite Statistical Methods - Lab 01"
author: "Maximilian Pfundstein (maxpf364) and Hector and Aashana and Lakshidaa"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
  html_document:
    df_print: paged
    number_sections: no
    toc: yes
    toc_float: no
---

```{r setup, include=FALSE}
library(gridExtra)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Describing Individual Variables

Consider the data set in the ``T1-9.dat`` file, National track records for women. For 55 different countries we have the national records for 7 variables (100, 200, 400, 800, 1500, 3000m and marathon ). Use R to do the following analyses.

## Describing the Variables

**Task:** Describe the 7 variables with mean values, standard deviations e.t.c.

**Answer:** First we import, name and look at the track times.

```{r}

track_times = read.table("data/T1-9.dat")
colnames(track_times) = c("country", "100m", "200m", "400m",
                   "800m", "1500m", "3000m", "marathon")
head(track_times)

```

We see that the times for the 100m, 200m and 400m are in seconds, whereas the times for 1500m, 3000m and the marathon are measured in minutes. So first we have to adjust that.

```{r}

track_times[,5:8] = track_times[,5:8] * 60
head(track_times)

```


```{r}

track_times_mean = apply(track_times[,2:8], 2, mean)
track_times_median = apply(track_times[,2:8], 2, median)
track_times_sd = apply(track_times[,2:8], 2, sd)

track_times_mean
track_times_median
track_times_sd

```

We see that the mean exceeds the anticipated scaled times. This means that if the time for 100m is `r track_times_mean[1]`, one could assume that for 200m twice the time, `r track_times_mean[1] * 2` is needed. But due to human exhaustion we see that the means increase by more than that, in this case the mean for the 200m marathon is `r track_times_mean[2]`. For the median the effect is a bit weaker as it is not as heavily effected by outliers as the mean. Still the effect is visible. Also the deviations increase, here the described effect is even larger.

## Illustrate the Variables

**Task:** Illustrate the variables with different graphs (explore what plotting possibilities R has). Make sure that the graphs look attractive (it is absolutely necessary to look at the labels, font sizes, point types). Are there any apparent extreme values? Do the variables seem normally distributed? Plot the best fitting (match the mean and standard deviation, i.e. method of moments) Gaussian density curve on the data’s histogram. For the last part you may be interested in the `hist()` and `density()` functions.

**Answer:**

```{r, echo=FALSE}

p1 = ggplot() +
  geom_histogram(aes(x = track_times$`100m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[1], color="Mean")) +
  geom_vline(aes(xintercept = track_times_median[1], color ="Median")) +
  labs(title = "Histrogram of 100m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p2 = ggplot() +
  geom_histogram(aes(x = track_times$`200m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[2], color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[2], color = "Median")) +
  labs(title = "Histrogram of 200m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p3 = ggplot() +
  geom_histogram(aes(x = track_times$`400m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[3], color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[3], color = "Median")) +
  labs(title = "Histrogram of 400m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p4 = ggplot() +
  geom_histogram(aes(x = track_times$`800m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[4],  color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[4],  color = "Median")) +
  labs(title = "Histrogram of 800m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p5 = ggplot() +
  geom_histogram(aes(x = track_times$`1500m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[5], color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[5], color = "Median")) +
  labs(title = "Histrogram of 1500m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p6 = ggplot() +
  geom_histogram(aes(x = track_times$`3000m`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[6], color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[6], color = "Median")) +
  labs(title = "Histrogram of 3000m",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

p7 = ggplot() +
  geom_histogram(aes(x = track_times$`marathon`, y=..density..),
                 color = "black", fill = "#dedede", bins = sqrt(nrow(track_times))) +
  geom_vline(aes(xintercept = track_times_mean[7], color = "Mean")) +
  geom_vline(aes(xintercept = track_times_median[7], color = "Median")) +
  labs(title = "Histrogram of marathon",
       y = "Density",
       x = "Time", color = "Legend") +
  scale_color_manual(values = c("#FFC300", "#C25E5E")) +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, p5, p6, p7, ncol = 2)

```


# Relationships Between the Variables

## Covariance and Correlation Matrices

**Task:** Compute the covariance and correlation matrices for the 7 variables. Is there any apparent structure in them? Save these matrices for future use.

**Answer:**

## Scatterplots

**Task:** Generate and study the scatterplots between each pair of variables. Any extreme values?

**Answer:**

## More Graphs

**Task:** Explore what other plotting possibilities R offers for multivariate data. Present other (at least two) graphs that you find interesting with respect to this data set.

**Answer:**

# Examining for Extreme Values

## Most Extreme Countries

**Task:** Look at the plots (esp. scatterplots) generated in the previous question. Which 3–4 countries appear most extreme? Why do you consider them extreme?

**Answer:**

## Multivariate Residual

One approach to measuring "extremism" is to look at the distance (needs to be defined!) between an observation and the sample mean vector, i.e. we look how far one is from the average. Such a distance can be called an *multivariate residual* for the given observation.

**Task:** The most common residual is the Euclidean distance between the observation and sample mean vector, i.e.

$$d(\vec{x}, \bar{x}) = \sqrt{(\vec{x} - \bar{x})^T(\vec{x} - \bar{x})}.$$
This distance can be immediately generalized to the $L^r, r > 0$ distance as

$$d_{L^r} (\vec{x} - \bar{x}) = \left( \sum_{i=1}^p | \vec{x}_i - \bar{x}_i |^r \right) ^{1/r},$$
where $p$ is the dimension of the observation (here $p = 7$).

Compute the squared Euclidean distance (i.e. r = 2) of the observation from the sample mean for all 55 countries using R’s matrix operations. First center the raw data by the means to get $\vec{x} - \bar{x}$ for each country. Then do a calculation with matrices that will result in a matrix that has on its diagonal the requested squared distance for each country. Copy this diagonal to a vector and report on the five most extreme countries. In this questions you **MAY NOT** use any loops.

**Answer:**

## Squared Distance

**Task:** The different variables have different scales so it is possible that the distances can be dominated by some few variables. To avoid this we can use the squared distance

$$d^2_{\boldsymbol{V}}(\vec{x} - \bar{x}) = (\vec{x} - \bar{x}) \boldsymbol{V}^{-1} (\vec{x} - \bar{x}),$$

where **V** is a diagonal matrix with variances of the appropriate variables on the diagonal. The effect, is that for each variable the squared distance is divided by its variance and we have a scaled independent distance. It is simple to compute this measure by standardizing the raw data with both means (centring) and standard deviations (scaling), and then compute the Euclidean distance for the normalized data. Carry out these computations and conclude which countries are the most extreme ones. How do your conclusions compare with the unnormalized ones?

**Answer:**

## Mahalanobis Distance

**Task:** The most common statistical distance is the *Mahalanobis distance*

$$d^2_M(\vec{x} - \bar{x}) = (\vec{x} - \bar{x})^T \boldsymbol{C}^{-1}(\vec{x} - \bar{x}),$$

where **C** is the sample covariance matrix calculated from the data. With this measure we also use the relationships (covariances) between the variables (and not only the marginal variances as $d_V(\cdot , \cdot)$ does). Compute the Mahalanobis distance, which countries are most extreme now?

**Answer:**

## Czekanowski’s Diagram

**Task:** Compare the results in b)–d). Some of the countries are in the upper end with all the measures and perhaps they can be classified as extreme. Discuss this. But also notice the different measures give rather different results (how does Sweden behave?). Summarize this graphically. Produce Czekanowski’s diagram using e.g. the `RMaCzek` package. In case of problems please describe them.

**Answer:**

# Source Code

```{r, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, results = 'show'}

```