---
title: "Multivariate Statistical Methods - Lab 02"
author: "Lakshidaa Saigiridharan (laksa656)"
date: "11/30/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Loading required libraries
library(ggplot2)
library("car") # for qqPlot()
library(heplots)
library(GGally)
```

# Question 1: Test of outliers

## Consider again the data set from the T1-9.dat file, National track records for women. In the first assignment we studied different distance measures between an observation and the sample average vector. The most common multivariate residual is the Mahalanobis distance and we computed this distance for all observations.

```{r}
# Loading required data
track_data <- read.table("T1-9.dat")
colnames(track_data) = c("country", "100m", "200m", "400m",
                   "800m", "1500m", "3000m", "marathon")
track_data
```

## a) The Mahalanobis distance is approximately chi???square distributed, if the data comes from a multivariate normal distribution and the number of observations is large. Use this chi??? square approximation for testing each observation at the 0.1% significance level and conclude which countries can be regarded as outliers. Should you use a multiple???testing correction procedure? Compare the results with and without one. Why is (or maybe is not) 0.1% a sensible significance level for this task?

The Mahalanobis distance is represented as,
$$d^2_M(\vec{x} - \bar{x}) = (\vec{x} - \bar{x})^T \boldsymbol{C}^{-1}(\vec{x} - \bar{x}),$$
where **C** is the sample covariance matrix calculated from the data.

```{r}
track_mat_data <- as.matrix(track_data[,2:8])
rownames(track_mat_data) <- track_data[,1]
#track_mat_data <- scale(track_mat_data, scale = FALSE)
n <- nrow(track_mat_data)

sample_variance = function(X) {
  
  X = as.matrix(X)
  
  identity = diag(nrow(X))
  one_n = matrix(1, nrow=nrow(X), ncol=1)
  
  inter =  identity - 1/nrow(X) * (one_n %*% t(one_n)) 
  
  return(1/nrow(X) * (t(X) %*% inter %*% X))
}

mahalanobis_distance = function(X) {
  X = as.matrix(X)
  
  V = sample_variance(X)
  ident = matrix(1, nrow=nrow(X), ncol=nrow(X))
  mu = 1/nrow(X) * (t(ident) %*% X)
  
  X_centered = X - mu
  
  return(diag(X_centered %*% solve(V) %*% t(X_centered)))
}

m_dist <- mahalanobis_distance(track_data[,2:8])
rownames(track_mat_data[which(m_dist > qchisq(0.99, ncol(track_mat_data))),])

alpha = 0.001
outlier_indices = which(1 - pchisq(m_dist, ncol(track_data) - 1) < alpha)
track_data$country[outlier_indices]

```

## b) One outlier is North Korea. This country is not an outlier with the Euclidean distance. Try to explain these seemingly contradictory results.

```{r}

euclidean_distance <- function(data, X){
  X <- as.matrix(X)
  n <- nrow(X)
  p <- ncol(X)
  matrix_1 <- matrix(rep(1,n*n), ncol = n, nrow = n)
  sample_mean <- t(X) %*% matrix_1 / n

  d <- X - t(sample_mean)
  dist <- d %*% t(d)
  rownames(dist) <- X[,1]

  diagonal <- sqrt(diag(dist))
  diag_df <- data.frame(Country = data[,1], Diag_value = diagonal)
  diag_df[order(diag_df$Diag_value, decreasing = TRUE),]
}

euc_dist <- euclidean_distance(track_data, track_data[,2:8])

cat("Countries sorted in decreasing order of their euclidean distance : \n")
euc_dist

cat("Position of North Korea in this list : ", which(euc_dist$Country == "KORN"))
```

On computing the Euclidean distance of the countries, it can be seen that North Korea is not an outlier. This is because the Euclidean distance computation treats data equally. Whereas, the Mahalanobis distance computation takes the covariance of each variable and obtains the correlation among variables. 

# Question 2: Test, confidence region and confidence intervals for a mean vector

## Look at the bird data in file T5-12.dat and solve Exercise 5.20 of Johnson, Wichern. Do not use any extra R package or built???in test but code all required matrix calculations. You MAY NOT use loops!

```{r}
# Importing the required data
bird_data <- read.table("T5-12.DAT")
colnames(bird_data) <- c("Tail_length", "Wing_length")
head(bird_data)
```

## a) Find and sketch the 95% confidence ellipse for the population means $\mu_1$ and $\mu_2$. Suppose it is known that $\mu_1 = 190$ mm and $\mu_2 = 275$ mm for male hook-billed kites. Are these plausible values for the mean tail length and mean wing length for the female birds? Explain.

```{r}
alpha <- 0.05

# Given mu_0 values (Center of ellipse)
mu_1 <- 190
mu_2 <- 275
mu_0 <- c(mu_1, mu_2)

n <- nrow(bird_data)
p <- ncol(bird_data)

x_bar <- colMeans(bird_data)
S <- sample_variance(bird_data)
S_inverse <- solve(S)

eigen_values <- eigen(S)$values
eigen_vectors <- eigen(S)$vectors

chi_sq <- qchisq(alpha, p)
c <- sqrt(chi_sq)   # Expression (4-8)

# Half lengths
major_len <- sqrt(eigen_values[1]) * c    # length of major axes
minor_len <- sqrt(eigen_values[2]) * c    # length of minor axes

# plot(mu_0[1], mu_0[2], type="n", main="test draw.ellipse")
# draw.ellipse(x=eigen_vectors,a=minor_len, b=major_len,angle=c(45,0))

```

## b) Construct the simultaneous 95% $T^2$ intervals for $\mu_1$ and $\mu_2$ and the 95% Bonferroni intervals for $\mu_1$ and $\mu_2$.Compare the two sets of intervals. What advantage, if any, do the $T^2$ intervals have over the Bonferroni intervals?

```{r}

alpha <- 0.05
# 95% T^2 intervals
t2_interval1 <- c(0,0)
t2_interval1[1] <- x_bar[1] - sqrt((p*(n-1)/(n-p)) * 
                                     qf(alpha, p, n-p)) * sqrt(S[1,1]/n)
t2_interval1[2] <- x_bar[1] + sqrt((p*(n-1)/(n-p)) * 
                                     qf(alpha, p, n-p)) * sqrt(S[1,1]/n)

t2_interval2 <- c(0,0)
t2_interval2[1] <- x_bar[2] - sqrt((p*(n-1)/(n-p)) * 
                                     qf(alpha, p, n-p)) * sqrt(S[2,2]/n)
t2_interval2[2] <- x_bar[2] + sqrt((p*(n-1)/(n-p)) * 
                                     qf(alpha, p, n-p)) * sqrt(S[2,2]/n)

cat(paste("95% T^2 intervals for mu_1 : ", "[", t2_interval1[1], ", ",
          t2_interval1[2], "]\n"))
cat(paste("95% T^2 intervals for mu_2 : ", "[", t2_interval2[1], ", ",
          t2_interval2[2], "] \n"))

# 95% Bonferroni intervals
bon_interval1 <- c(0,0)
bon_interval1[1] <- x_bar[1] - abs(qt(alpha/(2*p), n-1))*sqrt(S[1,1]/n)
bon_interval1[2] <- x_bar[1] + abs(qt(alpha/(2*p), n-1))*sqrt(S[1,1]/n)

bon_interval2 <- c(0,0)
bon_interval2[1] <- x_bar[2] - abs(qt(alpha/(2*p), n-1))*sqrt(S[2,2]/n)
bon_interval2[2] <- x_bar[2] + abs(qt(alpha/(2*p), n-1))*sqrt(S[2,2]/n)

cat(paste("\n95% Bonferroni intervals for mu_1 : ", "[", bon_interval1[1], ", ",
          bon_interval1[2], "]\n"))
cat(paste("95% Bonferroni intervals for mu_2 : ", "[", bon_interval2[1], ", ",
          bon_interval2[2], "]"))

```

## c) Is the bivariate normal distribution a viable population model? Explain with reference to Q-Q plots and a scatter diagram.

```{r}

ggplot(bird_data) + 
  geom_point(aes(x = Tail_length, y = Wing_length)) +
  ggtitle("Scatterplot of birds wing and tail size") +
  theme_light()

qqPlot(bird_data$Tail_length, main = "Q-Q Plot Tail Length") 
qqPlot(bird_data$Wing_length, main = "Q-Q Plot Wing Length")

```

# Question 3: Comparison of mean vectors (one???way MANOVA)

## We will look at a data set on Egyptian skull measurements (published in 1905 and now in heplots R package as the object Skulls). Here observations are made from five epochs and on each object the maximum breadth (mb), basibregmatic height (bh), basialiveolar length (bl) and nasal height (nh) were measured.

## a) Explore the data first and present plots that you find informative.

```{r}
# Loading the required data
skull_data <- Skulls
head(skull_data)

#pairs(skull_data, pch = ".", cex = 1.5)
ggpairs(skull_data, columns = 2:5)
```

## b) Now we are interested whether there are differences between the epochs. Do the mean vectors differ? Study this question and justify your conclusions.

```{r}

# epochs <- unique(skull_data$epoch)
# epoch1 <- skull_data[which(skull_data$epoch == epochs[1]),]
# epoch2 <- skull_data[which(skull_data$epoch == epochs[2]),]
# epoch3 <- skull_data[which(skull_data$epoch == epochs[3]),]
# epoch4 <- skull_data[which(skull_data$epoch == epochs[4]),]
# epoch5 <- skull_data[which(skull_data$epoch == epochs[5]),]

mb1 <- skull_data$mb
bh1 <- skull_data$bh
bl1 <- skull_data$bl
nh1 <- skull_data$nh

one_manova <- manova(cbind(mb1, bh1, bl1, nh1) ~ epoch, data=skull_data)
summary(one_manova)
one_manova$residuals

```

## c) If the means differ between epochs compute and report simultaneous confidence intervals. Inspect the residuals whether they have mean 0 and if they deviate from normality (graphi- cally).

```{r}

# k and l are the groups
# i is the feature

ci_custom = function(data, p, g, n, W, l, k, i, alpha = 0.05) {
  
  # Define groups
  group = unique(data$epoch)[i]
  group_data = data[data$epoch == group,]
  group_k = unique(data$epoch)[k]
  group_l = unique(data$epoch)[l]
  group_data_k = data[data$epoch == group_k,]
  group_data_l = data[data$epoch == group_l,]
  
  n_k = nrow(group_data_k)
  n_l = nrow(group_data_l)
  
  x_i = mean(group_data_k[,i+1]) - mean(group_data_l[,i+1])
  t_crit = qt(alpha/(p*g*(g-1)), df = (n-g))

  var_i = sqrt(diag(W)[i]/(n-g) * (1/n_k + 1/n_l)) 
  abs_i = abs(t_crit * var_i)
  
  res = c(x_i - abs_i, x_i + abs_i)
  names(res) = c("lower", "upper")
  
  return(res)
}

scite = function(data, i) {
  
  # Static Parameters
  p = ncol(data) - 1
  g = length(unique(Skulls$epoch))
  n = nrow(data)
  
  # Calculation of W
  
  W = 0
  
  for (group in unique(Skulls$epoch)) {
    
    group_data = Skulls[Skulls$epoch == group,]
    S = sample_variance(group_data[,2:5])
    W = W + (nrow(group_data) - 1) * S
  }
  
  df = data.frame()
  
  for (k in 2:length(unique(Skulls$epoch))) {
    for (l in 1:max((k-1), 1)) {
      row = c(i, k, l, ci_custom(data=data, p=p, g=g, n, W=W, l=l, k=k, i=i))
      df = rbind(df, row)
    }
  }

  colnames(df) = c("i", "k", "l", "lower", "upper")
  
  #res = ci_custom(data=data, p=p, g=g, n, W=W, l=1, k=3, i=2)
  
  return(df)
}

```
